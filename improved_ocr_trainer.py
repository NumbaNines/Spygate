#!/usr/bin/env python3\n\"\"\"\nImproved OCR Trainer with Fixed Data and Enhanced Preprocessing\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport os\nimport cv2\nimport numpy as np\nimport torch.nn.functional as F\nfrom datetime import datetime\nimport random\nfrom collections import Counter\n\nclass ImprovedCRNN(nn.Module):\n    def __init__(self, vocab_size: int, image_height: int = 64, rnn_hidden: int = 512):\n        super(ImprovedCRNN, self).__init__()\n        \n        # Enhanced feature extractor with more capacity\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d((2, 1), (2, 1)),\n            \n            nn.Conv2d(512, 512, kernel_size=2, padding=0),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n        )\n        \n        # Enhanced LSTM with more capacity\n        self.sequence_processor = nn.LSTM(\n            input_size=512,\n            hidden_size=rnn_hidden,  # Increased to 512\n            num_layers=2,\n            bidirectional=True,\n            dropout=0.2,  # Increased dropout\n            batch_first=True\n        )\n        \n        self.output_classifier = nn.Linear(rnn_hidden * 2, vocab_size)\n        \n    def forward(self, input_tensor):\n        cnn_features = self.feature_extractor(input_tensor)\n        \n        batch_size, channels, height, width = cnn_features.size()\n        cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, width))\n        cnn_features = cnn_features.squeeze(2)\n        cnn_features = cnn_features.permute(0, 2, 1)\n        \n        sequence_output, _ = self.sequence_processor(cnn_features)\n        \n        predictions = self.output_classifier(sequence_output)\n        predictions = F.log_softmax(predictions, dim=2)\n        \n        return predictions\n\nclass ImprovedDataset(Dataset):\n    def __init__(self, samples, character_mapping, height=64, width=256):\n        self.samples = samples\n        self.char_map = character_mapping\n        self.height = height\n        self.width = width\n    \n    def enhance_dark_text_region(self, image):\n        \"\"\"Enhanced preprocessing for dark text regions.\"\"\"\n        \n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image.copy()\n        \n        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n        enhanced = clahe.apply(gray)\n        \n        # Apply gamma correction to brighten dark regions\n        gamma = 1.5  # Brighten\n        enhanced = np.power(enhanced / 255.0, 1.0 / gamma) * 255.0\n        enhanced = enhanced.astype(np.uint8)\n        \n        # Apply bilateral filter to reduce noise while preserving edges\n        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)\n        \n        # Apply morphological operations to clean up text\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n        enhanced = cv2.morphologyEx(enhanced, cv2.MORPH_CLOSE, kernel)\n        \n        return enhanced\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        sample = self.samples[index]\n        \n        img_path = sample['image_path']\n        full_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        \n        # Extract region\n        x1, y1 = sample['bbox_x1'], sample['bbox_y1']\n        x2, y2 = sample['bbox_x2'], sample['bbox_y2']\n        region = full_img[y1:y2, x1:x2]\n        \n        # Apply enhanced preprocessing\n        img = self.enhance_dark_text_region(region)\n        \n        # Resize\n        img = cv2.resize(img, (self.width, self.height))\n        \n        # Normalize\n        img = img.astype(np.float32) / 255.0\n        img_tensor = torch.FloatTensor(img).unsqueeze(0)\n        \n        # Encode text\n        text = sample['ground_truth_text']\n        text_indices = [self.char_map[char] for char in text]\n        text_tensor = torch.LongTensor(text_indices)\n        \n        return img_tensor, text_tensor, len(text_indices)\n\ndef improved_collate_function(batch):\n    images, texts, text_lengths = zip(*batch)\n    \n    images = torch.stack(images, 0)\n    \n    from torch.nn.utils.rnn import pad_sequence\n    text_lengths = torch.tensor(text_lengths, dtype=torch.long)\n    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n    \n    return images, texts, text_lengths\n\ndef run_improved_training():\n    print(\"🚀 Improved OCR Training - Fixed Data & Enhanced Preprocessing\")\n    print(\"=\" * 70)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    print(f\"🔒 Training ID: {timestamp}\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"🎯 Device: {device}\")\n    \n    # Load FIXED data\n    with open('madden_ocr_training_data_FIXED.json', 'r') as f:\n        fixed_data = json.load(f)\n    \n    print(f\"📊 Fixed samples: {len(fixed_data):,}\")\n    \n    # Create vocabulary\n    unique_characters = set()\n    for sample in fixed_data:\n        text = sample['ground_truth_text']\n        unique_characters.update(text)\n    \n    sorted_chars = sorted(list(unique_characters))\n    \n    char_to_idx = {'<CTC_BLANK>': 0}\n    for i, char in enumerate(sorted_chars):\n        char_to_idx[char] = i + 1\n    \n    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n    \n    print(f\"📊 Vocabulary: {len(sorted_chars)} characters\")\n    print(f\"📊 Total classes: {len(char_to_idx)}\")\n    \n    # Split data\n    random.shuffle(fixed_data)\n    split_point = int(0.85 * len(fixed_data))\n    train_samples = fixed_data[:split_point]\n    val_samples = fixed_data[split_point:]\n    \n    print(f\"📊 Train samples: {len(train_samples):,}\")\n    print(f\"📊 Val samples: {len(val_samples):,}\")\n    \n    # Create datasets\n    train_dataset = ImprovedDataset(train_samples, char_to_idx)\n    val_dataset = ImprovedDataset(val_samples, char_to_idx)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=improved_collate_function)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=improved_collate_function)\n    \n    # Initialize IMPROVED model\n    vocab_size = len(char_to_idx)\n    model = ImprovedCRNN(vocab_size, rnn_hidden=512).to(device)  # Increased capacity\n    \n    print(f\"✅ Improved model initialized with {vocab_size} classes\")\n    \n    # IMPROVED training setup\n    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)  # AdamW with weight decay\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.00001)  # Cosine annealing\n    \n    print(f\"🔧 IMPROVED training settings:\")\n    print(f\"  - Learning Rate: 0.0001\")\n    print(f\"  - Optimizer: AdamW with weight decay\")\n    print(f\"  - Scheduler: Cosine Annealing\")\n    print(f\"  - Batch Size: 8\")\n    print(f\"  - Enhanced Preprocessing: ✅\")\n    print(f\"  - Fixed Data: ✅\")\n    \n    # Create save directory\n    save_dir = f\"models/improved_ocr_{timestamp}\"\n    os.makedirs(save_dir, exist_ok=True)\n    print(f\"💾 Save directory: {save_dir}\")\n    \n    # Training loop\n    best_validation_loss = float('inf')\n    patience_counter = 0\n    max_patience = 20\n    \n    print(\"\\n🚀 Starting IMPROVED training...\")\n    print(\"=\" * 70)\n    \n    for epoch in range(100):\n        # Training phase\n        model.train()\n        epoch_train_loss = 0.0\n        valid_train_batches = 0\n        \n        for batch_idx, (images, texts, text_lengths) in enumerate(train_loader):\n            images = images.to(device)\n            texts = texts.to(device)\n            text_lengths = text_lengths.to(device)\n            \n            optimizer.zero_grad()\n            \n            predictions = model(images)\n            predictions_ctc = predictions.permute(1, 0, 2)\n            input_lengths = torch.full((predictions_ctc.size(1),), predictions_ctc.size(0), dtype=torch.long, device=device)\n            \n            loss = criterion(predictions_ctc, texts, input_lengths, text_lengths)\n            \n            if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() < 100.0:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n                optimizer.step()\n                \n                epoch_train_loss += loss.item()\n                valid_train_batches += 1\n            \n            if batch_idx % 50 == 0:\n                print(f\"Epoch {epoch+1:2d}, Batch {batch_idx:3d}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n        \n        # Validation phase\n        model.eval()\n        epoch_val_loss = 0.0\n        valid_val_batches = 0\n        \n        with torch.no_grad():\n            for images, texts, text_lengths in val_loader:\n                images = images.to(device)\n                texts = texts.to(device)\n                text_lengths = text_lengths.to(device)\n                \n                predictions = model(images)\n                predictions_ctc = predictions.permute(1, 0, 2)\n                input_lengths = torch.full((predictions_ctc.size(1),), predictions_ctc.size(0), dtype=torch.long, device=device)\n                \n                loss = criterion(predictions_ctc, texts, input_lengths, text_lengths)\n                \n                if not torch.isnan(loss) and not torch.isinf(loss):\n                    epoch_val_loss += loss.item()\n                    valid_val_batches += 1\n        \n        # Calculate averages\n        avg_train_loss = epoch_train_loss / max(valid_train_batches, 1)\n        avg_val_loss = epoch_val_loss / max(valid_val_batches, 1)\n        \n        # Learning rate scheduling\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        print(f\"Epoch {epoch+1:2d} - Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n        \n        # Save best model\n        if avg_val_loss < best_validation_loss:\n            best_validation_loss = avg_val_loss\n            patience_counter = 0\n            \n            model_path = os.path.join(save_dir, \"best_improved_model.pth\")\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'validation_loss': avg_val_loss,\n                'epoch': epoch,\n                'char_to_idx': char_to_idx,\n                'idx_to_char': idx_to_char,\n                'training_id': timestamp,\n                'vocab_size': vocab_size,\n                'improvements_applied': True,\n                'enhanced_preprocessing': True,\n                'fixed_data': True\n            }, model_path)\n            \n            print(f\"💾 NEW BEST MODEL! Val Loss: {avg_val_loss:.4f}\")\n            \n            if avg_val_loss <= 0.2:  # Lower target\n                print(f\"🎯 Target loss 0.2 reached! Stopping training.\")\n                break\n        else:\n            patience_counter += 1\n            print(f\"⏳ No improvement. Patience: {patience_counter}/{max_patience}\")\n            \n            if patience_counter >= max_patience:\n                print(f\"🛑 Early stopping. Best val loss: {best_validation_loss:.4f}\")\n                break\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(f\"✅ IMPROVED training completed!\")\n    print(f\"🏆 Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"💾 Model saved in: {save_dir}\")\n    print(\"=\" * 70)\n\nif __name__ == \"__main__\":\n    run_improved_training()
