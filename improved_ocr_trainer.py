#!/usr/bin/env python3\n\"\"\"\nImproved OCR Trainer with Fixed Data and Enhanced Preprocessing\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport os\nimport cv2\nimport numpy as np\nimport torch.nn.functional as F\nfrom datetime import datetime\nimport random\nfrom collections import Counter\n\nclass ImprovedCRNN(nn.Module):\n    def __init__(self, vocab_size: int, image_height: int = 64, rnn_hidden: int = 512):\n        super(ImprovedCRNN, self).__init__()\n        \n        # Enhanced feature extractor with more capacity\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d((2, 1), (2, 1)),\n            \n            nn.Conv2d(512, 512, kernel_size=2, padding=0),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n        )\n        \n        # Enhanced LSTM with more capacity\n        self.sequence_processor = nn.LSTM(\n            input_size=512,\n            hidden_size=rnn_hidden,  # Increased to 512\n            num_layers=2,\n            bidirectional=True,\n            dropout=0.2,  # Increased dropout\n            batch_first=True\n        )\n        \n        self.output_classifier = nn.Linear(rnn_hidden * 2, vocab_size)\n        \n    def forward(self, input_tensor):\n        cnn_features = self.feature_extractor(input_tensor)\n        \n        batch_size, channels, height, width = cnn_features.size()\n        cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, width))\n        cnn_features = cnn_features.squeeze(2)\n        cnn_features = cnn_features.permute(0, 2, 1)\n        \n        sequence_output, _ = self.sequence_processor(cnn_features)\n        \n        predictions = self.output_classifier(sequence_output)\n        predictions = F.log_softmax(predictions, dim=2)\n        \n        return predictions\n\nclass ImprovedDataset(Dataset):\n    def __init__(self, samples, character_mapping, height=64, width=256):\n        self.samples = samples\n        self.char_map = character_mapping\n        self.height = height\n        self.width = width\n    \n    def enhance_dark_text_region(self, image):\n        \"\"\"Enhanced preprocessing for dark text regions.\"\"\"\n        \n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image.copy()\n        \n        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n        enhanced = clahe.apply(gray)\n        \n        # Apply gamma correction to brighten dark regions\n        gamma = 1.5  # Brighten\n        enhanced = np.power(enhanced / 255.0, 1.0 / gamma) * 255.0\n        enhanced = enhanced.astype(np.uint8)\n        \n        # Apply bilateral filter to reduce noise while preserving edges\n        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)\n        \n        # Apply morphological operations to clean up text\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n        enhanced = cv2.morphologyEx(enhanced, cv2.MORPH_CLOSE, kernel)\n        \n        return enhanced\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        sample = self.samples[index]\n        \n        img_path = sample['image_path']\n        full_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        \n        # Extract region\n        x1, y1 = sample['bbox_x1'], sample['bbox_y1']\n        x2, y2 = sample['bbox_x2'], sample['bbox_y2']\n        region = full_img[y1:y2, x1:x2]\n        \n        # Apply enhanced preprocessing\n        img = self.enhance_dark_text_region(region)\n        \n        # Resize\n        img = cv2.resize(img, (self.width, self.height))\n        \n        # Normalize\n        img = img.astype(np.float32) / 255.0\n        img_tensor = torch.FloatTensor(img).unsqueeze(0)\n        \n        # Encode text\n        text = sample['ground_truth_text']\n        text_indices = [self.char_map[char] for char in text]\n        text_tensor = torch.LongTensor(text_indices)\n        \n        return img_tensor, text_tensor, len(text_indices)\n\ndef improved_collate_function(batch):\n    images, texts, text_lengths = zip(*batch)\n    \n    images = torch.stack(images, 0)\n    \n    from torch.nn.utils.rnn import pad_sequence\n    text_lengths = torch.tensor(text_lengths, dtype=torch.long)\n    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n    \n    return images, texts, text_lengths\n\ndef run_improved_training():\n    print(\"üöÄ Improved OCR Training - Fixed Data & Enhanced Preprocessing\")\n    print(\"=\" * 70)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    print(f\"üîí Training ID: {timestamp}\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"üéØ Device: {device}\")\n    \n    # Load FIXED data\n    with open('madden_ocr_training_data_FIXED.json', 'r') as f:\n        fixed_data = json.load(f)\n    \n    print(f\"üìä Fixed samples: {len(fixed_data):,}\")\n    \n    # Create vocabulary\n    unique_characters = set()\n    for sample in fixed_data:\n        text = sample['ground_truth_text']\n        unique_characters.update(text)\n    \n    sorted_chars = sorted(list(unique_characters))\n    \n    char_to_idx = {'<CTC_BLANK>': 0}\n    for i, char in enumerate(sorted_chars):\n        char_to_idx[char] = i + 1\n    \n    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n    \n    print(f\"üìä Vocabulary: {len(sorted_chars)} characters\")\n    print(f\"üìä Total classes: {len(char_to_idx)}\")\n    \n    # Split data\n    random.shuffle(fixed_data)\n    split_point = int(0.85 * len(fixed_data))\n    train_samples = fixed_data[:split_point]\n    val_samples = fixed_data[split_point:]\n    \n    print(f\"üìä Train samples: {len(train_samples):,}\")\n    print(f\"üìä Val samples: {len(val_samples):,}\")\n    \n    # Create datasets\n    train_dataset = ImprovedDataset(train_samples, char_to_idx)\n    val_dataset = ImprovedDataset(val_samples, char_to_idx)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=improved_collate_function)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=improved_collate_function)\n    \n    # Initialize IMPROVED model\n    vocab_size = len(char_to_idx)\n    model = ImprovedCRNN(vocab_size, rnn_hidden=512).to(device)  # Increased capacity\n    \n    print(f\"‚úÖ Improved model initialized with {vocab_size} classes\")\n    \n    # IMPROVED training setup\n    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)  # AdamW with weight decay\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.00001)  # Cosine annealing\n    \n    print(f\"üîß IMPROVED training settings:\")\n    print(f\"  - Learning Rate: 0.0001\")\n    print(f\"  - Optimizer: AdamW with weight decay\")\n    print(f\"  - Scheduler: Cosine Annealing\")\n    print(f\"  - Batch Size: 8\")\n    print(f\"  - Enhanced Preprocessing: ‚úÖ\")\n    print(f\"  - Fixed Data: ‚úÖ\")\n    \n    # Create save directory\n    save_dir = f\"models/improved_ocr_{timestamp}\"\n    os.makedirs(save_dir, exist_ok=True)\n    print(f\"üíæ Save directory: {save_dir}\")\n    \n    # Training loop\n    best_validation_loss = float('inf')\n    patience_counter = 0\n    max_patience = 20\n    \n    print(\"\\nüöÄ Starting IMPROVED training...\")\n    print(\"=\" * 70)\n    \n    for epoch in range(100):\n        # Training phase\n        model.train()\n        epoch_train_loss = 0.0\n        valid_train_batches = 0\n        \n        for batch_idx, (images, texts, text_lengths) in enumerate(train_loader):\n            images = images.to(device)\n            texts = texts.to(device)\n            text_lengths = text_lengths.to(device)\n            \n            optimizer.zero_grad()\n            \n            predictions = model(images)\n            predictions_ctc = predictions.permute(1, 0, 2)\n            input_lengths = torch.full((predictions_ctc.size(1),), predictions_ctc.size(0), dtype=torch.long, device=device)\n            \n            loss = criterion(predictions_ctc, texts, input_lengths, text_lengths)\n            \n            if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() < 100.0:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n                optimizer.step()\n                \n                epoch_train_loss += loss.item()\n                valid_train_batches += 1\n            \n            if batch_idx % 50 == 0:\n                print(f\"Epoch {epoch+1:2d}, Batch {batch_idx:3d}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n        \n        # Validation phase\n        model.eval()\n        epoch_val_loss = 0.0\n        valid_val_batches = 0\n        \n        with torch.no_grad():\n            for images, texts, text_lengths in val_loader:\n                images = images.to(device)\n                texts = texts.to(device)\n                text_lengths = text_lengths.to(device)\n                \n                predictions = model(images)\n                predictions_ctc = predictions.permute(1, 0, 2)\n                input_lengths = torch.full((predictions_ctc.size(1),), predictions_ctc.size(0), dtype=torch.long, device=device)\n                \n                loss = criterion(predictions_ctc, texts, input_lengths, text_lengths)\n                \n                if not torch.isnan(loss) and not torch.isinf(loss):\n                    epoch_val_loss += loss.item()\n                    valid_val_batches += 1\n        \n        # Calculate averages\n        avg_train_loss = epoch_train_loss / max(valid_train_batches, 1)\n        avg_val_loss = epoch_val_loss / max(valid_val_batches, 1)\n        \n        # Learning rate scheduling\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        print(f\"Epoch {epoch+1:2d} - Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n        \n        # Save best model\n        if avg_val_loss < best_validation_loss:\n            best_validation_loss = avg_val_loss\n            patience_counter = 0\n            \n            model_path = os.path.join(save_dir, \"best_improved_model.pth\")\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'validation_loss': avg_val_loss,\n                'epoch': epoch,\n                'char_to_idx': char_to_idx,\n                'idx_to_char': idx_to_char,\n                'training_id': timestamp,\n                'vocab_size': vocab_size,\n                'improvements_applied': True,\n                'enhanced_preprocessing': True,\n                'fixed_data': True\n            }, model_path)\n            \n            print(f\"üíæ NEW BEST MODEL! Val Loss: {avg_val_loss:.4f}\")\n            \n            if avg_val_loss <= 0.2:  # Lower target\n                print(f\"üéØ Target loss 0.2 reached! Stopping training.\")\n                break\n        else:\n            patience_counter += 1\n            print(f\"‚è≥ No improvement. Patience: {patience_counter}/{max_patience}\")\n            \n            if patience_counter >= max_patience:\n                print(f\"üõë Early stopping. Best val loss: {best_validation_loss:.4f}\")\n                break\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(f\"‚úÖ IMPROVED training completed!\")\n    print(f\"üèÜ Best validation loss: {best_validation_loss:.4f}\")\n    print(f\"üíæ Model saved in: {save_dir}\")\n    print(\"=\" * 70)\n\nif __name__ == \"__main__\":\n    run_improved_training()
